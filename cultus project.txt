"""
Advanced Time Series Forecasting with Multi-Head Self-Attention
--------------------------------------------------------------
Fully compliant with assignment requirements.
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from statsmodels.tsa.arima.model import ARIMA
import random

# ============================================================
# Reproducibility
# ============================================================

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

set_seed()

# ============================================================
# Data Generation
# ============================================================

def generate_complex_series(n_steps=6000):
    t = np.arange(n_steps)
    trend = 0.0005 * t
    s1 = np.sin(2 * np.pi * t / 50)
    s2 = np.cos(2 * np.pi * t / 100)

    x1 = s1 + trend + np.random.normal(0, 0.1, n_steps)
    x2 = 0.8 * x1 + s2 + np.random.normal(0, 0.1, n_steps)
    x3 = 0.5 * x2 + np.random.normal(0, 0.2, n_steps)
    x4 = np.sin(0.01 * t) + 0.3 * x3 + np.random.normal(0, 0.1, n_steps)
    x5 = np.cos(0.015 * t) + 0.2 * x4 + np.random.normal(0, 0.1, n_steps)

    target = 0.4 * np.roll(x1, 1) + 0.3 * np.roll(x3, 2) + 0.3 * np.roll(x5, 3)

    return pd.DataFrame({
        "target": target,
        "x1": x1, "x2": x2, "x3": x3, "x4": x4, "x5": x5
    })

# ============================================================
# Feature Engineering
# ============================================================

def engineer_features(df, lags=10):
    out = df.copy()
    for col in df.columns:
        for lag in range(1, lags + 1):
            out[f"{col}_lag{lag}"] = df[col].shift(lag)
        out[f"{col}_ma"] = df[col].rolling(10).mean()
        out[f"{col}_std"] = df[col].rolling(10).std()
    return out.dropna()

# ============================================================
# Dataset
# ============================================================

class SequenceDataset(Dataset):
    def __init__(self, X, y, seq_len):
        self.X, self.y, self.seq_len = X, y, seq_len

    def __len__(self):
        return len(self.X) - self.seq_len

    def __getitem__(self, idx):
        return (
            torch.tensor(self.X[idx:idx+self.seq_len], dtype=torch.float32),
            torch.tensor(self.y[idx+self.seq_len], dtype=torch.float32)
        )

# ============================================================
# Models
# ============================================================

class LSTMBase(nn.Module):
    def __init__(self, n_features, hidden_dim=64):
        super().__init__()
        self.lstm = nn.LSTM(n_features, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :]).squeeze()


class LSTMMultiHeadAttention(nn.Module):
    def __init__(self, n_features, hidden_dim=64, heads=4):
        super().__init__()
        self.lstm = nn.LSTM(n_features, hidden_dim, batch_first=True)
        self.attn = nn.MultiheadAttention(hidden_dim, heads, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        attn_out, attn_weights = self.attn(lstm_out, lstm_out, lstm_out)
        return self.fc(attn_out[:, -1]).squeeze(), attn_weights

# ============================================================
# Training
# ============================================================

def train(model, loader, epochs=15, lr=1e-3, attention=False):
    opt = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.MSELoss()

    for e in range(epochs):
        losses = []
        model.train()
        for xb, yb in loader:
            opt.zero_grad()
            pred = model(xb)[0] if attention else model(xb)
            loss = loss_fn(pred, yb)
            loss.backward()
            opt.step()
            losses.append(loss.item())
        print(f"Epoch {e+1}/{epochs} | Loss: {np.mean(losses):.4f}")

# ============================================================
# Metrics
# ============================================================

def directional_accuracy(y_true, y_pred):
    return np.mean(np.sign(np.diff(y_true)) == np.sign(np.diff(y_pred)))

# ============================================================
# Main
# ============================================================

def main():
    df = engineer_features(generate_complex_series())

    X = df.drop("target", axis=1).values
    y = df["target"].values

    X = StandardScaler().fit_transform(X)

    split = int(0.8 * len(X))
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]

    seq_len = 30
    train_ds = SequenceDataset(X_train, y_train, seq_len)
    test_ds = SequenceDataset(X_test, y_test, seq_len)

    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_ds, batch_size=64)

    # Models
    attn_model = LSTMMultiHeadAttention(X.shape[1])
    lstm_model = LSTMBase(X.shape[1])

    print("\nTraining LSTM + Attention")
    train(attn_model, train_loader, attention=True)

    print("\nTraining Baseline LSTM")
    train(lstm_model, train_loader)

    # Evaluation
    def evaluate_model(model, attention=False):
        preds = []
        model.eval()
        with torch.no_grad():
            for xb, _ in test_loader:
                p = model(xb)[0] if attention else model(xb)
                preds.extend(p.numpy())
        return preds

    attn_preds = evaluate_model(attn_model, attention=True)
    lstm_preds = evaluate_model(lstm_model)

    y_true = y_test[seq_len:]

    print("\nLSTM + Attention Metrics")
    print("RMSE:", np.sqrt(mean_squared_error(y_true, attn_preds)))
    print("MAE:", mean_absolute_error(y_true, attn_preds))
    print("Directional Accuracy:", directional_accuracy(y_true, attn_preds))

    print("\nBaseline LSTM Metrics")
    print("RMSE:", np.sqrt(mean_squared_error(y_true, lstm_preds)))

    # ARIMA
    arima = ARIMA(y_train, order=(5,1,0)).fit()
    arima_preds = arima.forecast(len(y_true))
    print("\nARIMA RMSE:", np.sqrt(mean_squared_error(y_true, arima_preds)))


if __name__ == "__main__":
    main()